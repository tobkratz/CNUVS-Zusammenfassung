\section{Transport Layer}
	In Chapter 2 beschrieben, funktioniert Routing von Packeten global am besten Abstrackt. Jedoch gibt es Anwendungen und Fälle, in denen man eine direkte Verbindung zwischen 2 Applications auf je einem Host herstellen will. Der Sender teilt Nachrichten der Application in Segmente und schickt diese weiter an den Network Layer, um zum Ziel geroutet zu werden. Im Network Layer des Empfängers werden die Segmente dann zu Nachrichten zusammengebaut und an die App weitergereicht. \\
	Im Transport Layer wird die logische Verbindung zwischen Prozessen hergestellt, während im Vergleich der Network Layer die logische Verbindung zwischen Hosts herstellt. Im Internet sind Beispiele für Protokolle des Transport Layers TCP \& UDP. 
	\begin{itemize}
		\item TCP
			\begin{itemize}
				\item Congestion Control
				\item Flow Control
				\item connection Setup \& teardown  (COTS)
			\end{itemize}
		\item UDP  
			\begin{itemize}
				\item connectionless
				\item Best-effort implemtierung von IP auf dem Transport Layer
			\end{itemize}
	\end{itemize}
	Keines dieser Protokolle bietet jedoch eine Lösung für Delay \& Bandwidth guarantees.

	\subsection{Segmentation(+Blocking). Multiplexing, Addressing}
		Die theoretische beschränkung von IP Packeten sind 64k mit Header. TCP Streams sind zum Teil jedoch längern. Um Applications ein Verständnis zu geben, wie Packete verschickt und SDUs zusammengebaut werden, muss dieser Vorang transparent geschehen. Packete können über Multiplexing auf dem Host an alle Applications gesendet werden, und Applications holen sich nur die Packete, die sie brauchen. Das kommt jedoch mit einem großen Sicherheitsproblem, da nun Anwendungen wie Trojaner einfach den Traffic anderer Programme mithören. 

		\paragraph{Sockets and Ports} 
			Das OSI Modell sieht für die Kommunikation zwischen Programmen und zur identifizierung dieser die Nutzung von lokalen CEPs vor. In Systemen könnte der System PID zur identifizierung benutzt werden, der müsste jedoch für alle Hosts erreichbar sein, außerdem ändert sich dieser bei jedem Neustart des Prozesses oder des Hosts. Im Internet Modell hat man sich jedoch für die Einführung von Ports entschieden. Ein Prozess kann auf einen festen Port announcen oder nach einem zufälligen Port fragen. \\
			Eine Application ist dann durch einen socket (IP:Port Kombination) erreichbar, wobei die IP die des Empfänger Hosts und die Port Nummer die der Empfänger Application ist. Es gibt gewissen Ports, die für Anwendungen reserviert sind a.k.a. ''well-known'' Ports (IANA: 0-1023). Beispiele sind 22/tcp für ssh, 25/tcp für smtp oder 443/tcp für https. Prozesse die solche Ports öffnen wollen brauchen in der Regel root Privilegien. \\ 
			Um demultiplexing im Internet zu nutzen, empfängt der Host ein IP Datagram, bestehend aus source und destionation Socket. Diese Datagrams transportieren je ein Transport-Layer segment als Application Data. Das Problem hierbei: Wenn IP eines Tages ersetzt wird, funktionieren TCP/UDP nicht mehr, da die Nutzung von IP:Port hard in die implemtierung gecoded sind. 
	
	\subsection{Connection Controll}
		Connection Control funktioniert natürlich nur bei CO Protokollen, in unserem Beispiel also nur TCP. Wie schon in Kapitel 1 angeschnitten, gibt es 3 Phasen der einer connection:
		\begin{itemize}
			\item CONNECT (herstellen der Verbindung)
			\item DATA (Transfer von Daten)
			\item DISCONNECT (Schließen der Verbindung)
		\end{itemize}
		Um zwischen Network und Transport Layer Phasen zu unterscheiden, schreibt man dort jeweils ein N-- bzw. T- Prefix zu den Phasen also z.B. T-Connect oder N-Data.

		\subsubsection{Phases}  
			Die verschiedenen Schritte können confirmed oder unconfirmed ablaufen. T-Connect wird immer confirmed, T-Data wird unconfirmed übertragen (in seltenden Fällen kann das auch confirmed laufen) und T-Disconnect können confirmed und unconfirmed ablaufen. Ein Beispiel für ein Ablauf ist auf 4:20 zu finden. Auch ein State-Diagramm, dass die Übergänge zu verschiedenen State beschreibt ist auf 4:24 enthalten.

			\paragraph{Establishment}
				Beim T-Connect gibt es mehrere Methoden, die aufgerufen werden (können)
				\begin{itemize}
					\item T-Connect.Request(Destination Address, Source Address)
					\item T-Connect.Indication(Destination Address, Source Address)
					\item T-Connect.Response(Responding Address)
					\item T-Connect.Confirmation(Responding Address)
				\end{itemize}
				Wobei die Destination Address die Addresse des Transport Service Users ist, also die Addresse die zum Transport gerufen wird. Die Source Address ist die Addresse des aufrufenden Service Users und die Responding Address die Addresse des Antwortenden Service Users. 

			\paragraph{Data Transfer}
				\begin{itemize}
					\item T-Data.req(userdata)
					\item T-Data.ind(userdata)
				\end{itemize}
				Die userdata ist dabei die Payload, die von der Application transportiert werden soll. 

			\paragraph{Connection Release}
				Der Grund für einen Disconnect kann verschieden sein. Es kann zu einem Release kommen durch abruptes verlieren der Verbding oder als Folge des Connects. Das verlieren von TSDUs (T + SDU) ist möglich.
				\begin{itemize}
					\item T-Disconnect.req(userdata)
					\item T-Disconnect.ind(cause, userdata)
				\end{itemize}
				Der cause für einen Disconnect kann z.B. ein request vom remote User, Quality of service below minimum, error oder auch unknowm sein. Beispiele für Abläufe verschiedener teardowns sind auf 4:23 zu finden. 

		\subsubsection{Error Control}
			\paragraph{Error Handeling in CONNECT}
				Bleibt ein T-connect.req (CR) unbeantwortet, es kommt also zu einem Timeout, sendet der Host einfach erneut ein CR. Kommt das erste CR jedoch doch nur verspätet beim Empfänger an, und dieser Antwortet auf das erste CR mit einem CC (Connection confirmation) und bekommt dann das zweite CR, sollte durch einbeziehung des Application Layers der zweite CR unbeantwortet bleiben. Was passiert jedoch, wenn das CC verloren geht und wie geht man damit um? Denn der Empfänger (sender of CC) erwartet jetzt einen Datenaustausch. Eine Lösung wäre hier der Three-Way-Handshake

			\paragraph{Three-Way-Handshake}
				Beim Three-Way-Handshake Wird das durch ein T-Connect.rsp ausgelöste CC nochmals bestätigt. Das kann entweder direkt ber Data oder per ACK passieren. Das schützt jedoch nicht davor, dass wenn CC und ACK verloren gehen, der Empfänger nicht sagen kann, ob es sich um eine neue oder alte Anfrage handelt. Eine Lösung wäre die Einführung von Sequence Number ins CR, ACK und CC. Host A sendet eine CR (seq=x) and Host B. B antwortet mit einem ACK(seq=y,ack=x) und A sendet dann z.B. DATA(seq=x,ACK=y). Nur wenn die richtigen Sequence Numbers angehängt sind, findet dann der Datenverkehr statt. Anderer Fall. Es kommt ein alter CR bei B an, dieser Antwortet mit der seq Number, da er nicht weiß wie alt er ist. A empfängt ein ACK zu einem alten CR und sendet jedoch ein REJECT, da die Anfrage nicht mehr relevant ist. 
			
			\paragraph{Connection Rejection \& Release}
				Eine angefragte Verbindung kann jedoch auch Rejected werden. Antwortet ein Empfänger z.B. auf ein CR mit einem DR (Disconnect Request), antwortet der Sender dann mit einem DC (Disconnect Confirm). Normalerweise sendet A ein DR an B, hört jedoch auf eventuelle Daten, die noch unterwegs sein könnten. Er wartet also auf das ACK (DC) von seinem DR und kann währenddessen noch Daten Empfangen. Empfängt er ein DC, kann er sich sicher sein, dass B keine Daten danach mehr sendet. So ein Teardown nennt sich explicit. Ein Implicit Teardown wäre ein Teardown der Network Layer Connection. Das Ziel eines Release ist es, dass beide Seiten alle Daten Empfangen haben und sich nichts mehr mitzuteilen haben. Doch was passiert wenn ein DR/DC/DT verloren geht und nue gesendet werden muss? \\
				Wie kann man sicher gehen, dass A weiß was B weiß und B das weiß und A weiß dass B das weiß... 
		
		\subsubsection{Two Army Problem}
		Wie koordinieren sich zwei Armeen, die sich nur Boten durch Tal voller Feinde schicken können und wie stimmen sie ab, dass sie gemeinsam angreifen? Man kann sich nie sicher sein, dass die eigene Nachricht angekommen ist. Das ist das gleiche Problem wie beim connection Release, bei dem ist das Risiko jedoch nicht so hoch, und es können Risiken in Kauf genommen werden. Die Lösung für den Connection Release ist es, einenen Timer parallel zum DR zu starten. falls ein DC/ACK verloren geht, timeoutet die connection automatisch und beide Hosts wissen von der geschlossenen Verbindung. 

	\subsection{Flow Control}
		Bisher wurden Packete einfach per best Effort versendet. Problematisch wird es jedoch, wenn der Sender eine bessere Verbindung hat als der Empfänger. Wie kann der Sender sicher gehen ob und wenn ja welche Packete angekommen sind? Eine Bestätigung für jedes angekommene Packet vom Empfänger würde diesen eventuell noch mehr einschränken. Wird dieses Verfahren dennoch genutzt, kann der Sender einfach die Packete, die nicht geACKt wurden nach einer gewissen Zeit nochmal versenden, was den Empfänger jedoch nochmal mehr belastet. 
		Eine anderere Lösung ist Flow Control. Das Ziel ist es, langsame Empfänger vor schnellen Sendern zu schützen. Flow Control kann auf zwei Layern implementiert werden: Im Link Layer, dort wird einem Überfluss an "forwading segments" vorgegriffen. Auf höhreren Layern, z.B. auf dem Network oder Transport Layer wird vor einem Überfluss an Conections geschützt. Auf dem Link Layer ist die implementierung simpel, da Segmente eine feste Größe haben, anders auf dem Transport Layer, da können PDUs stark in der Größe variieren. 
		
		\paragraph{Buffer Allocation}
			Ein mit dem Flow Control starj zusammenhängendes Problem ist die Buffer Allocation. Ein Empfänger kann z.B. zwar nicht durch seinen Link eingeschränkt sein, sondern durch das Verarbeiten der Packete. Es wird dann ein Buffer geeigneter Größe erfordert, der eingehende Packete bis zu Bearbeitung/Weiterleitung zwischenspeichern kann. Um als Empfänger die Rate, in der Daten eingehen zu kontrollieren, gibt es verschiene Möglichkeiten. Der Empfänger verlangsamt den Sender, wenn es keinen freien Buffer Space mehr gibt (das kann explizit oder implizit geschehen). Dem kann man vorgreifen, indem der Sender initial Buffer Space anfragt und dieser dann allociert wird, oder der Empfänger announced "Ich habe noch X Buffer Space Verfügbar zur Zeit"
		
		\paragraph{Alternating-Bit-Protocol}
			Es wird angenommen, dass es genug freien Buffer Space nach dem Empfangen eines Packets gibt. Ein Empfänger sendet eine Bestätigung für jedes eingehende Packet. Der Sender warten nach jedem gesendeten Packet auf ein ACK des Empfängers. Der Empfänger sendet das ACK erst, wenn das Packet verarbeitet ist, er kann so also kontrollieren, wann und wie oft Packete eintreffen. Kommt ein ACK beim Sender nicht an, gibt es vier Fälle"
			\begin{enumerate}
				\item Packet loss
				\item ACK-loss
				\item ACK late (heavy Traffic)
				\item Ack late (Flow control)
			\end{enumerate}
			Der Sender kann jedoch nicht ziwschen den vier Fällen unterscheiden. Nach einer gewissen Zeit sendet er das DT PDU also nochmal. \\
			Das Alternating-Bit-Protocol ist in der Theorie eine gute Methode für Flow Control, aber eignet es sich auch in realen Umgebungen? In Einem Beispiel senden wir 8KBit von Frankfurt direkt nach NYC über einen 1GBit/s Link. Wir gehen davon aus, dass durch die Hosts kein Delay entsteht. Die Propagation durch die Verbindung ist
			$$
				T_{prop} \approx \frac{6200km}{300.000\frac{km}{s}} \approx 20ms
			$$ 
			Die Zeit, die das 8KBit Packet braucht, ist etwa
			$$
				T_{trans} \approx \frac{8KBit}{10^9\frac{Bit}{s}} \approx 8\mu s
			$$ 
			Weiteer nehmen wir an, das ein Ack etwa $1\mu s$ Transmission benötigt. Bis der Sender also 8Kbit übertragen hat und der Sender ein ACK PDU versendet hat und dieses beim Sender ankommt vergehen also $\approx 40.009ms$. Die Utilization des Links ist also etwa:
			$$
				U_{sender} = \frac{0,008}{40,009} \approx 0.0002
			$$
		
		\paragraph{Stop and Continue}
			Eine alternative einfache Möglichkeit ist Stop-and-Continue Methode. Der Sender sendet Daten, bis er vom Empfänger ein Stop bekommt. Er pausiert das senden bis zu einem continue Signal. Das Problem ist jedoch, dass es es bei einem überlasteten Host schwer werden kann, die Stop nachricht zu versenden, wenn es schon zu einem Overflow kommt. Auch kann es sein, dass der Sender Packete versendet, obwohl der Empfänger schon eine Stop Nachricht versendet hat, da die Nachricht noch nicht angekommen ist (Delay). Der Empfänger muss das Stop also schon früh genug versenden, um einen Overflow proaktiv zu vermeiden. Diese Methode funktioniert nur bei Full-Duplex Links. Ein Beispiel für die implementierung ist das XON/XOFF Protokoll.

		\subsubsection{Rate and Credit based Flow Control}
			Beim Rate based Flow Control können im Vergleich zum Stop and Continue Änderungen in der Rate angekündigt werden. Wenn sich also der maximal Verfügbare Buffer ändert kann der Empfänger das mitteilen. Jedoch auch hier gibt es das Problem, dass der Sender erst zu spät davon mitbekommt. Der Übergang zwischen Phasen kann anders als bei Stop and Continue flüssiger ablaufen, da der Sender nicht direkt den Empfang stoppen muss, sondern ihn auch nur einschränken kann. Es gibt jedoch keine Möglichkeit für den Sender sicherzustellen, dass Packete ankommen. \\
			Idee des Credit Based Flow Control ist es, dem Sender regelmäßig Credits zu geben, um Packete zu senden. Hat der Sender keine Credits mehr, pausiert er das senden, bis er wieder neue credits vom Empfänger bekommt. Alternativ kann der Sender auch einmalig eine absolute Anzahl bekanntgeben. \\
			Auch lassen sich die Konzepte mit einer Error correction kombinieren. Der Sender kann dem Empfänger "permitten", daten zu senden und er kann das empfangen von Daten "acknowledgen". Eine ACK Nachricht beduetet dabei, die Nachricht ist korrept angekommen. Der Sender kann also seinen timoute-timer sowie die lokale Kopie des Packets löschen. Problem ist jedoch, wenn durch Flow Control das ACK Packet ausbleibt. Wenn der Sender timeoutet, sendet er das Packet dann nochmal. \\
			Eine Protokoll des Credit Based Flow Control ist das Sliding Window Protokoll. Ein Beispiel ist auf 4:46. 
			
	\subsection{Error Control}
		Es kann nie garantiert werden, dass alle Packete ankommen und noch weniger, dass das immer erkannt wird. Wird jedoch erkannt, dass ein Packet verloren gegangen ist, gibt es bei den meisten Protokollen keine Reaktion. Das Packet wird einfach gedroppt. Meistens wird das durch eine Fehlerhafte Checksumme im Header erkannt. Da die Checksumme für den IP Header jedoch die gleiche wie für die Payload ist, ensteht die meiste Fehlererkennung an der Stelle, kann jedoch falsch Interpretiert werden. Ein Empfänger kann auch beim feststellen dass ein Packet Fehlerhaft war ein negative-ACK versenden und die neusendung anfordern. Das hat den Vorteil, dass dann nicht erst der timeout beim Sender auslaufen muss, bis das Packet neu verschickt wird. Wird ein Fehler z.B. durch eine falsche Sequence Number erkannt und ein NACK versendet, kann es jedoch sein, dass ein Packet über einen anderen Weg geroutet wurde, weil ein schnellerer gefunden wurde, und das vorherige Packet noch z.B. im Router hängt. Einzig verlässlig in dem Punkt ist das System: timeout @sender → resend. 

		\paragraph{Automatic Repeat reQuest (ARQ)}
			Der erste Typ ARQ war das Alternating Bit Protocol. Das lässt sich jedoch weiter verbessern: Go-back-N. Geht ein Packet z.B. mit der Sequence Number 2 verloren oder löst ein Error aus, sendet der Empfänger kein ACK für diesen Frame. Er sendet jedoch weiter die Folge Frames (z.B. 3-8), dessen Frames vom Empfänger Link jedoch weggeworfen werden, da dieser noch auf den Frame mit der Seq. Number 2 warten. Timeoutet dann der Frame2, wird dieser erneut vom Sender gesendet. der Empfänger empfängt diesen Frame korrekt und sendet dann ein ACK. währenddessen sendet der Sender die Folge Frames von 2 (3-8) ebenfalls nochmals da diese auch kein ACK bekommen haben. (Beispiel siehe 4:49). \\ Eine Adaption dieses Protokolls wäre, wenn der Empfänger nicht auf den Timeout wartet, sondern für den fehlerhaften Frame ein NACK sendet. Dieser Frame wird dann erneut gesendet, in der Zeit buffert der data link des Empfängers Frames mit dn folgende seq. Number.

	\subsection{Congestion Control}
		Bei Kommunikation zwischen zwei Hosts gibt es immer ein Bottleneck, dass die Geschwindigkeit beschränkt. Ein Netz ist dann auf genau dieses Bottleneck beschränkt. Werden mehr Daten gesendet, als das Bottleneck übertragen kann, kommt es zu einem congestion collapse. Bei solch einem collapse gehen viele Packete verloren und die Effizienz sinkt unter das Bottleneck. Je nachdem wo das Bottleneck ist, kann sich das ganze auch in sich selbst steigern, ist z.B. der Router durch den Buffer Space beschränkt und es kommt zu einem collapse wobei der Sender ein Protokoll nutzt, dass nicht angekommene Packete nochmal sendet, wird der Router mit einer noch größeren Flut an Packeten überschwemmt.
		Im folgenden sind $\lambda_{in}$ die Daten die Host A versendet und $\lambda_{out}$ die, die Host B empfängt. Im Idealfall ohne Packetverlust ist $\lambda_{in}=\lambda_{out}$
		\paragraph{Scenario 1}
			Wir gehen von einem Router mit umbegränztem Buffer aus und Packete werden nicht neu versendet. Die Throughput steigt bis zum Maximum an, bleibt dann stabil, der Delay steigt exponentiell an, wenn es zu einer congestion kommt.
		
		\paragraph{Scenario 2}
			Hat der Router jedoch einen beschränkten Buffer und Host A sendet nicht bestätigte Packete nach einiger Zeit nochmal. Host A sendet jetzt $\lambda'_{in}=\lambda_{in}$+ retransmitted packages. Perfekt wäre nur wirklich verloren gegangene Packete neu gesendet werden, $\lambda'_{in}$ wäre $>\lambda_{out}$. Werden nicht wirklich verlorerene Packete neu gesendet, wird $\lambda'_{in}$ hörher als nötig. 

		Wie bei den Scenarien gemerkt, wird congestion controll esentiell, um einen Schneeball Effekt zu verhindern. 
		
		\subsubsection{solving congestion Control}
			Eine Globale Lösung wäre, die Sende Rate an den Bottleneck anzupassen. Das kann jedoch nur gloabl passieren und jeder Router müsste seine Maximal Kapazität bekanntgeben. Diese Lösung macht das Problem nur komplizierter und kleine Konfigurationsfehler können das gesamte System betreffen. Deswegen wird Congestion Control zu einem lokalen Problem gemacht. Meistens wird es zwischen Sender um Empfänger gelöst. 
			Das Ziel ist es, so viele Packete wie möglich zu verschicken, ohne jedoch die Leitung zu verstopfen. Außerdem sollte Congestion Control Fair ablaufen, jedoch unter welchen Bedingungen? Ein 3-Hop Weg ist genausoviel Wert wie 3 1-Hop ways? Priorisierung in Anwendungen (Video Call > Downloads)? 

			\paragraph{Design Options for Congestion Control}
				Es gibt 2 fundamentale Anstätze, Congestion Control zu gewährleisten:
				\begin{itemize}
					\item Closed-Loop - Der Sender bekommt Feedback über das aktuelle Verhalten und kann die Kapazität anpassen. 
						\begin{itemize}
							\item implementierung am Sender
							\item implementierung am Empfänger
						\end{itemize}
					\item Open-Loop - Das System von vorne herein funktioniert und keine Korrekturen im Betrieb notwenig sind.
						\begin{itemize}
							\item explicit Feedback - Im Fall wenn es passiert, den Sender Informieren
							\item implizit Feedback - Der Sender passt die Rate selbst an ohne direktes Feedback vom Empfänger sondern durch Daten aus dem Betrieb (z.B. ausbleibende ACKs). Beispeil: TCP
						\end{itemize}
				\end{itemize}
			
			\paragraph{Possible Actions}
				Um Netze vor Congestion zu schützen, gibt es andere alternativen als einfach den Traffic zu pausieren. Um häufige Congestions entgegen zu wirken, kann die Netz Kapazität z.B. durch mehr Router erhöht werden. Auch kann ein Router das allocieren von zusätzlichem Traffic einschränken, wenn das Netz schon fast ausgelastet ist, jedoch sind Informationen über den Netzwerk state selten Verfügbar. Ebenso können alle Session einen Teil der Verfügbaren Load reduzieren, um allgemein mehr Kapazität zu schaffen. Dafür wird jedoch Netzwerk Feedback benötigt, das geht also nur in closed-loops. 
				Die Klassifizierung der Packete kann Zentral am Router oder am Host stattfinden. Im Internet geschieht das (außer beim droppen von Packeten) am End-Host. Die einzelnen Klassen bekommen entweder einen gewissen Byte Betrag pro Sekunde (rate based) oder eine gewisse Anzahl an Sequence Numbers/Bytes im Netz verarbeitet werden dürfen (Es gibt auch andere, jedoch unpopulärere Methoden z.B. Credit Based Congestion Control).
			
			\paragraph{Package Dropping}
				Ensteht ein collapse durch einen vollen Router Buffer, müssen Packete aus diesem gedroppt werden, doch welche? Eine Methode ist die drop-tail queue, bei dir das neuse Packet weggeworfen wird. Das spielt z.B. gut mit dem go-back-n Protokoll zusammen. Für andere Anwendungen, z.B. Telefon sind neue Packete wichtiger als alte, hier würden am besten Packete gedroppt, die schon lange in der Queue sind. Eine solche Aktion ist auch immer, ein implizites Feedback. Der Sender erkennt das nicht ankommen des Packets. Ensteht eine Congestion durch einen vollen Buffer, muss der Traffic verringert werden. 
			
			\paragraph{proactiv Actions}
				Prinzipiell ist ein Router nicht mehr in der Lage, richtig zu funktionieren, wenn er einmal eine volle Queue hat, es sollte also um jeden Preis vermieden werden, z.B. indem man bereits bei z.B. 90\% einen warning state Aufruft. Eine andere Methode ist das Chocke Packet. Ein Router mit vollen Buffer sendet so ein solches Packet an den Sender, dass diesem sagt, die Rate zu verringern. Das Problem ist jedoch, in einem congested Netz gehen mehr Packete verloren als sonst, es kann also sein, dass der Sender ein solches Packet nie erhält. Auch kann ein Router statt ein extra Packet zu senden, ein warning Bit in allen ausgehenden Packeten setzen, dass allen Hosts sagt, dass der Router überlastet ist, oder das kurz bevor steht. Eine vielleicht etwas radikalere Lösung ist die Random Early Detection (RED), die mit einer Rate zufällig Packete droppt, auch wenn der Router noch nicht überlastet ist, um diesen genau davor zu schützen. Diese Rate je nach Anzahl Packete im Buffer variieren. 

			\paragraph{Feedback Actions}
				Sobald ein Sender Informationen über eine congestion erreicht hat, muss der Traffic reduziert werden. Doch wie das geschieht, hängt von den Protokollen des Transport Layers ab.

	\subsection{(Internet) UDP}
		\paragraph{RFC 768} 
			UDP ist eine Best Effort Lösung für Packetübertragung im Internet. UDP ist connectionless, es gibt also keine Handshakes zwischen sender \& Empfänger. UDP Packete können verloren gehen und ohne congestion Control einfach ''in die Welt'' geblasen werden. UDP wird für z.B. Streaming benutzt, wo es nicht so schlimm ist, wenn ein Packet mal wegfällt, aber auch DNS Anfragen oder SNMP setzten auf UDP. In-Oder delivery oder reliability können auf dem Application Layer implementiert werden. Ein UDP Segment ist wie folgt aufgebaut:
			\begin{table}[h]
				\centering
				\begin{tabular}{|l|l|l|l|l|l|l|l|}
				\hline
				\multicolumn{4}{|l|}{source Port \#} & \multicolumn{4}{l|}{dest Port \#} \\ \hline
				\multicolumn{4}{|l|}{length}         & \multicolumn{4}{l|}{checksum}     \\ \hline
				\multicolumn{8}{|l|}{Application data (message)}        				 \\
				\multicolumn{8}{|l|}{}                                                   \\
				\multicolumn{8}{|l|}{}                                                   \\
				\multicolumn{8}{|l|}{}                                                   \\ \hline
				\end{tabular}
			\end{table}

		\paragraph{UDP Checksum}
			Das Ziel ist es, Fehler in übertragenen Daten zu erkennen, z.B. geflippte Bits. Der Sender teilt für die UDP Checksum in 16 Bit Sequenzen. Er addiert dann jewils die Sequenzen zusammen und nimmt davon das 1er Komplement und schreibt das in das Checksum Feld. Der Empfänger muss dann nur die Segemnte wieder zusammen addieren und die Checksumme draufrechnen. Ist das Ergebnis 1 (0xFFFF), ist das Packet wahrscheinlich Fehlerfrei, ansonsten wird das Packet gedroppt. Ein Beispiel für eine solche Berechnung ist auf Folie 4:79.

	\subsection{(Internet) TCP}
		\subsubsection{Overview}
			Im Gegensatz zu UDP hat TCP einige Eigentschaften, die es geeigneter machen für den gebrauchen in den meisten Netzen. 
			\begin{itemize}
				\item \textbf{Point-to-Point} Es gibt einen Sender und einen Empfänger
				\item \textbf{Reliable, in-order byte stream} Es gibt keine direkten Nachrichten Grenzen
				\item \textbf{Pipelined} TCP Congestion und FLow Control setzt auf window-sized Version
				\item \textbf{Full duplex data} Bi-directional communication in der selben Verbindung möglich. Es gibt eine maximum segment size (MSS)
				\item \textbf{send \& reciever Buffer} Es gibt im T-Layer (unter dem Socket zum A-Layer) einen TCP Buffer, der auch die zu sendenden Plakate buffert
				\item \textbf{Connection-Oriented Communication} Vor dem Übertragen von Daten werden Kontroll Nachrichten ausgetauscht (Handshake), die den State initiieren
				\item \textbf{Flow \& congestion Control} In TCP sind implementierungen für diese beiden Ansätze vorhanden
			\end{itemize}
			Ein TCP Segment hat dabei den volgenden Aufbau:
			\begin{table}[h]
				\centering
				\begin{tabular}{|c|l|l|l|l|l|l|l|c|l|l|l|l|l|l|l|}
				\hline
				\multicolumn{8}{|c|}{source Port \#}                              & \multicolumn{8}{c|}{dest Port \#}    \\ \hline
				\multicolumn{16}{|c|}{sequence Number\footnote[1]}                                                                 \\ \hline
				\multicolumn{16}{|c|}{acknowledgement number\footnote[2]}                                                                 \\ \hline
				\multicolumn{1}{|l|}{head len} & not used & U\footnote[3] & A \footnote[4] & P \footnote[5] & R \footnote[6] & S \footnote[7] & F\footnote[8] & \multicolumn{8}{c|}{receive window \footnote[9]} \\ \hline
				\multicolumn{8}{|c|}{checksum\footnote[10]}                                   & \multicolumn{8}{c|}{Urg data pnter} \\ \hline
				\multicolumn{16}{|c|}{Options (variable length)}                                                       \\ \hline
				\multicolumn{16}{|l|}{}                                                                                \\
				\multicolumn{16}{|c|}{application data}                                                                \\
				\multicolumn{16}{|c|}{variable length}                                                                 \\
				\multicolumn{16}{|l|}{}                                                                                \\ \hline
				\end{tabular}
			\end{table}

			\footnotetext[1]{Es wird nach anzahl der Bytes gezählt, nicht der Segment}
			\footnotetext[2]{Es wird nach anzahl der Bytes gezählt, nicht der Segmente}
			\footnotetext[3]{URG: urgend data. In der Regel nicht genutzt}
			\footnotetext[4]{ACK: ACK \# ist valid}
			\footnotetext[5]{PSH: Push Data now. In der Regel nicht genutzt}
			\footnotetext[6]{RST, connection Establishment/teardown commands}
			\footnotetext[7]{SYN, connection Establishment/teardown commands}
			\footnotetext[8]{FIN, connection Establishment/teardown commands}
			\footnotetext[9]{\# Bytes, die der Reciever bereit ist, zu akzeptieren}
			\footnotetext[10]{Internet Checksumme (vgl. UDP)}
			
			Der Socket eines Host, z.B. ein Webserver auf Port :443 kann dabei mehrere connections gleichzeitig haben. Beim TCP (CO) De-multiplexing wird dann zwischen dest und source Socket unterschieden. Dieser TCP hat dann genau 4-Tupel:
			\begin{itemize}
				\item Source IP Address
				\item Source Port Number
				\item Dest IP Address
				\item Dest Port Number
			\end{itemize}
			Ein Empfänger nutzt diese 4 Tupel um ein Packet/Segment dem richtigen Socket zuzuweisen. WEbserver haben z.B. für jede Verbindung zu einem client einen eigenen Socket. Gewisse Webserver können auch für jede HTTP Anfrage einen eigenen Socket brauchen. 

		\subsubsection{Error \& Connection Control in TCP}
			Sequence Numbers werden bei TCP genauso wie ACKs genutzt. Ein ACK gibt dabei die Sequence Number des nächsten erwarteten Packets an. Ein Beispiel für einen Ablauf ist auf 4:87. \\
			Um TCP timeouts richtig einzustellen, also die Zeit bis zu einem timeout, gibt es einige Faktoren zu beachten. Der Timeout darf nicht kleiner als die Round Trip Time (RTT) sein, sonst kommt es zu unnötigen retransmission, sie darf jedoch auch nicht zu lange sein, sonst kommt es zu unnötig langen verzögerungen beim neusenden von Packeten. Die RTT kann man z.B. berechnen, indem man die Zeit ziwschen senden eines Packets und ankommen des ACKs misst. Dieses verfahren nennt man SampleRTT. Da das von Packet zu Packet varrieren kann, kann man das Ergebnis durch runden mehrerer Rechnungen glätten. \\
			TCP Connections können aktiv oder passiv hergestellt werden:
			\begin{itemize}
				\item \textbf{Active Mode:} Ein Host fragt eine Connection bei einem anderem Host an. 
				\item \textbf{Passive Mode:} Eine Anwenung Informiert TCP, dass sie auf einem Socket connections annimmt. Es kann jedoch auch Port unspeziefisch sagen, dass alle eingehenden Verbindungen akzeptiert werden. Sobald eine Verbindung eingeht, wird ein neuen Socket aufgemacht, der als Endpunkt für die Verbindung gilt. 
			\end{itemize}
			Der Ablauf einer TCP Connection ist auf 4:90 Beispielhaft aufgeführt, außerdem sind auf 4:91 \& 4:92 Szenarien aufgeführt, wie sich TCP in einigen Fällen verhält, wenn ACKS ausbleiben, nach timeouts ankommen oder anderes. 

			\paragraph{Fast Retransmit}
				Geht ein Packet verloren, kann es lange dauern, bis der timeout ausläuft, und das Packet neu gesendet wird. Eine Lösung für dieses Problem ist Fast Retransmit. Erhält ein Sneder aufeinanderfolgend 3 mal das gleiche ACK, da zwar nachfolgende Packete angekommen sind, der Empfänger jedoch noch das verlorene Packet erwartet sendet TCP schon vor einem timeout das Packet neu. Das ist guter Kompromiss ziwschen nicht unnötig lange warten und nicht unnötig Packete versenden. 
			
			\paragraph{Send and Receive Buffers in TCP}
				TCP pflegt jeweils auf Sender und Empfänger Seite einen Buffer um beim Sender Packete zu speicher, die z.B. noch nicht geackt sind, um diese ggf. neu zu senden oder Daten der Application zu speichern, bevor diese versendet werden. Ältere Versionen von TCP haben nach dem Go-Back-N Prinzip gearbeitet und out-of-order Packete weggeworfen. Inzwischen werden auf Empfänger Seite Packete, die neuer als erwartet sind ziwschengespeichert, um nicht alle neu senden zu müssen. 

		\subsubsection{Flow \& congestion Control in TCP}
			In TCP kann der Empfänger den advertise size von seinem Buffer announcen. Der Buffer size ist dabei: 
			\begin{quote}
				Advertised window = MaxRcvdBuffer–((NextByteExpected-1) –LastByteRead)
			\end{quote}
			der advertiste Speicher beschränkt dabei den Sender in der Anzahl an Daten, die dieser verschickt. Der Sender achtet dabei darauf, dass 
			\begin{quote}
				LastByteSent–LastByteAcked $\le$ AdvertisedWindow
			\end{quote}
			bzw. \begin{quote}
				EffectiveWindow = AdvertisedWindow–(LastByteSent–LastByteAcked)
			\end{quote}
			
			\paragraph{Self Clocking Winwo}
				Bisher gingen wir davon aus, dass neue Packete diregt verschickt werden, wenn ein ACK eingetroffen ist. Problem jedoch, wenn ACKs schneller einkommen, als die Application Daten in den send Buffer schreibt, werden viele kleine Packete verschickt. Man spricht dabei vom \frQuo silly window syndrom\frQuc. Eine Lösung hierfür ist Nagle's Algorithmus. \\
				Wenn die Verfügbaren Daten und das advertiste Window $\ge$ MSS sind, wird ein volles Segment verschickt. Ansonsten wird geschaut, ob bereits an Packet verschickt wurde, das noch ncith geACKt wurde, dann wird gewartet bis MSS voll ist. Falls kein Packet verschickt ist, werden die neuen Daten sofort verschickt. 
			
			\paragraph{Congestion Control}
				TCP kontrollliert arbeitet mit implicit feedback um congestion zu verhindern, konkret mit der Information über dropped Packages. Es kann weiter nicht unterschiden werden, warum Packete gedroppt werden, deswegen wird davon ausgegangen, dass diese wegen congestion gedroppt wurden. TCP ermittelt mit diesen ein congestion window, dass im laufenden Betrieb reflektiert und aktualisiert wird. Zusätzlich zu den Limits der Flow Control beschränkt der Sender die Anzahl an Packeten durch 
				\begin{quote}
					LastByteSent-LastByteAcked $\le$ CongWin
				\end{quote}
				Um die darstellung von congestion Control zu vereinfachen, wird in Zukunft das flow Control Window ignoriert. \\
				Hat TCP jetzt genau so viele Packete in das Netzwerk gesendet, wie das congestion Window erlaubt, wird gewartet, bis diese das Netz wieder verlassen haben. Durch das erreichen von ACKs ist sichergestellt, dass die Packete nicht mehr im Netz sind und es können neue gesendet werden. 
			\paragraph{Congestion Window (AIMD)}
				Bekommt TCP ein ACK, geht es nach Greedy von der Annahme aus, dass das Netz noch Kapazität hat und erhöht das congestion window. Bleiben jedoch ACKs aus und es kommt zu einem timeout, wird das congestion window verkleinert. Doch um wieviel? Um das Netz vor einem congestion collapse zu schützen, müssen drastische Maßnahmen getroffen werden, und das congestion window wird auf 50\% gekürzt (halbiert)! Das Window kann jedoch nie kleiner als 1 sein. \\
				Das kürzt jedoch die nehr als nötig, wenn Packete nicht wegen congestion, sondern z.B. durch Übertragungsfehler gedroppt werden. Das passiert in physischen Netzen selten, ist jedoch grade bei Mobilen Netzen ein Problem. Das ganze nennt sich mupliplicative decrease. \\
				Wird jedoch das Congestion Window durch ACKs erhöhrt, wird es nicht so stark erhöht, da es sonst zu einem collapse kommt. TCP testet sich langsam an das Maximum heran und erhöht es immer nur um einen kleinen Teil. Wenn alle gesendeten Packete innerhalb der RTT (Round Trip time) ein ACK erhalten haben, sendet TCP in Zukunft ein Packet mehr pro RTT. \\
				Das ganze kann auch per additive increase passieren, dabei wird das Congestion Window pro erreichtes ACK um folgenden Wert erhöht:
				\begin{quote}
					Congestion Window += MSS x (MSS / Congestion Window)
				\end{quote}
				TCP setzt insgesamt auf ein AIMD (adaptive increase multiplicative decrease) ''Sägezahn'' Pattern. (Beispiel siehe 4:105). AIMD start jedoch mit einem congestion Window von 1 oder 2 und brauch länger, um anzulaufen, was gerade bei schnellen Netzen unnötigt Zeit kostet. Es kann also anfangs das congestion Window z.B. je ACK verdoppelt werden, bis es einmal durch loss halbiert wird. Ab diesem Punkt wird dann auf linearen anstieg umgestellt. 
			\paragraph{Packet Burst}
				Ein Problem jedoch: Ein Packet geht verloren. Nach einem timout wird das Packet neu gesendet und es kommt ein ACK zurück. Packete wurden seit dem timeout zurückgehalten und das congestion window zwar halbiert. jedoch wird jetzt auf einmal das gesamate (halbierte) Congestion Window auf einmal ausgeschöpft und ein ''Packet Sturm'' losgeschickt. Dadurch wird es wahrscheinlich zu einem hohen Packetverlust kommen. Die Lösung ist, in dem Fall wieder slow zu starten mit einem congestion window von 1. Jedoch gibt es jetzt eine Idee von der maximalen Größe des Congestion Windows. Dieser wird kann also congestion threshold genutzt werden. Es wird also mit dem exponentiellen slow start bis zum threashold increased und dann auf additive increase gewechselt. \\
				Ein Beispiel Ablauf ist auf 4:111.
	
	\subsection{Other Protocols}
		Da der Multimedia, vorallem Video, streaming in den letzten Jahren stark zugenommen hat, wird mehr auf Protokolle, die auf UDP Basis aufbauen gesetzt. Der meiste Traffic geschieht trotzdem größtenteils mit TCP, andere Protokolle z.B. QUIC sind auf dem vormarsch. 

		\subsubsection{SCTP}
			Das Stream Control Transmission Protocol (SCTP) Protokoll ist message-Oriented (wie UDP), hat jedoch wie TCP flow, congestion und error control implementiert. Jede Nachticht in einem Stream erhält dabei eine sequence Number, um Packete zu ordern. Ein Empfänger kann dabei ordering ordering und reliability an und ausschalten, ebenso fragmentation und Nagle. Außerdem können mehrere Streams in einer einzelenen Verbindung gehandelt werden und beide Endpunkt können mehrere IP Addresses nutzen (failover).
		
		\subsubsection{DCCP}
			DCCP (DatagramCongestionControl Protocol) ist im Kern etwa wie TCP nur ohne bytestream semantik und reliability oder wie UDP mit congestion Control, Handshakes und optionalen ACKs. Es ist CO. Durch seperate Header und Data checksums ist es nicht zuverlässig und wird selten genutzt. Es besteht keine große Chance, dass es weitfläufig genutzt wird.
		
		\subsubsection{RTP/RTCP}
			RTP (Real-time Transfer Protocol) ist gedacht als realtime multimedia Protokoll. RTP Session Data unterstützt sequence Number und timestams, encryption und Informationen über den Payload inhalt. Ähnlich wie QUIC läuft RTP auf Basis von UDP. Dabei unterstützt die RTP library für den Application Layer Funktionen wie Port numbers and IP Addressen, Payload type identifizierung, Packet sequence numbering und time-stamping. 
		
		\subsection{QUIC}
			QUIC (Quick UDP Internet Connections) wurde von Google entwickelt, ist inziwschen aber in IETF Standarts eingearbeitet. 2017 machte es bereits etwa 7\% des globalen Traffics aus. Durch schnelles connection setup and secutiry mit TSL1.3 wird es besonders attraktiv gemacht. Es unterstützt außerdem error forwading.