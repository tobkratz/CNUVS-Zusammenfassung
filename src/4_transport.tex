\section{Transport Layer}
	In Chapter 2 beschrieben, funktioniert Routing von Packeten global am besten Abstrackt. Jedoch gibt es Anwendungen und Fälle, in denen man eine direkte Verbindung zwischen 2 Applications auf je einem Host herstellen will. Der Sender teilt Nachrichten der Application in Segmente und schickt diese weiter an den Network Layer, um zum Ziel geroutet zu werden. Im Network Layer des Empfängers werden die Segmente dann zu Nachrichten zusammengebaut und an die App weitergereicht. \\
	Im Transport Layer wird die logische Verbindung zwischen Prozessen hergestellt, während im Vergleich der Network Layer die logische Verbindung zwischen Hosts herstellt. Im Internet sind Beispiele für Protokolle des Transport Layers TCP \& UDP. 
	\begin{itemize}
		\item TCP
			\begin{itemize}
				\item Congestion Control
				\item Flow Control
				\item connection Setup \& teardown  (COTS)
			\end{itemize}
		\item UDP  
			\begin{itemize}
				\item connectionless
				\item Best-effort implemtierung von IP auf dem Transport Layer
			\end{itemize}
	\end{itemize}
	Keines dieser Protokolle bietet jedoch eine Lösung für Delay \& Bandwidth guarantees.

	\subsection{Segmentation(+Blocking). Multiplexing, Addressing}
		Die theoretische beschränkung von IP Packeten sind 64k mit Header. TCP Streams sind zum Teil jedoch längern. Um Applications ein Verständnis zu geben, wie Packete verschickt und SDUs zusammengebaut werden, muss dieser Vorang transparent geschehen. Packete können über Multiplexing auf dem Host an alle Applications gesendet werden, und Applications holen sich nur die Packete, die sie brauchen. Das kommt jedoch mit einem großen Sicherheitsproblem, da nun Anwendungen wie Trojaner einfach den Traffic anderer Programme mithören. 

		\paragraph{Sockets and Ports} 
			Das OSI Modell sieht für die Kommunikation zwischen Programmen und zur identifizierung dieser die Nutzung von lokalen CEPs vor. In Systemen könnte der System PID zur identifizierung benutzt werden, der müsste jedoch für alle Hosts erreichbar sein, außerdem ändert sich dieser bei jedem Neustart des Prozesses oder des Hosts. Im Internet Modell hat man sich jedoch für die Einführung von Ports entschieden. Ein Prozess kann auf einen festen Port announcen oder nach einem zufälligen Port fragen. \\
			Eine Application ist dann durch einen socket (IP:Port Kombination) erreichbar, wobei die IP die des Empfänger Hosts und die Port Nummer die der Empfänger Application ist. Es gibt gewissen Ports, die für Anwendungen reserviert sind a.k.a. ''well-known'' Ports (IANA: 0-1023). Beispiele sind 22/tcp für ssh, 25/tcp für smtp oder 443/tcp für https. Prozesse die solche Ports öffnen wollen brauchen in der Regel root Privilegien. \\ 
			Um demultiplexing im Internet zu nutzen, empfängt der Host ein IP Datagram, bestehend aus source und destionation Socket. Diese Datagrams transportieren je ein Transport-Layer segment als Application Data. Das Problem hierbei: Wenn IP eines Tages ersetzt wird, funktionieren TCP/UDP nicht mehr, da die Nutzung von IP:Port hard in die implemtierung gecoded sind. 
	
	\subsection{Connection Controll}
		Connection Control funktioniert natürlich nur bei CO Protokollen, in unserem Beispiel also nur TCP. Wie schon in Kapitel 1 angeschnitten, gibt es 3 Phasen der einer connection:
		\begin{itemize}
			\item CONNECT (herstellen der Verbindung)
			\item DATA (Transfer von Daten)
			\item DISCONNECT (Schließen der Verbindung)
		\end{itemize}
		Um zwischen Network und Transport Layer Phasen zu unterscheiden, schreibt man dort jeweils ein N-- bzw. T- Prefix zu den Phasen also z.B. T-Connect oder N-Data.

		\subsubsection{Phases}  
			Die verschiedenen Schritte können confirmed oder unconfirmed ablaufen. T-Connect wird immer confirmed, T-Data wird unconfirmed übertragen (in seltenden Fällen kann das auch confirmed laufen) und T-Disconnect können confirmed und unconfirmed ablaufen. Ein Beispiel für ein Ablauf ist auf 4:20 zu finden. Auch ein State-Diagramm, dass die Übergänge zu verschiedenen State beschreibt ist auf 4:24 enthalten.

			\paragraph{Establishment}
				Beim T-Connect gibt es mehrere Methoden, die aufgerufen werden (können)
				\begin{itemize}
					\item T-Connect.Request(Destination Address, Source Address)
					\item T-Connect.Indication(Destination Address, Source Address)
					\item T-Connect.Response(Responding Address)
					\item T-Connect.Confirmation(Responding Address)
				\end{itemize}
				Wobei die Destination Address die Addresse des Transport Service Users ist, also die Addresse die zum Transport gerufen wird. Die Source Address ist die Addresse des aufrufenden Service Users und die Responding Address die Addresse des Antwortenden Service Users. 

			\paragraph{Data Transfer}
				\begin{itemize}
					\item T-Data.req(userdata)
					\item T-Data.ind(userdata)
				\end{itemize}
				Die userdata ist dabei die Payload, die von der Application transportiert werden soll. 

			\paragraph{Connection Release}
				Der Grund für einen Disconnect kann verschieden sein. Es kann zu einem Release kommen durch abruptes verlieren der Verbding oder als Folge des Connects. Das verlieren von TSDUs (T + SDU) ist möglich.
				\begin{itemize}
					\item T-Disconnect.req(userdata)
					\item T-Disconnect.ind(cause, userdata)
				\end{itemize}
				Der cause für einen Disconnect kann z.B. ein request vom remote User, Quality of service below minimum, error oder auch unknowm sein. Beispiele für Abläufe verschiedener teardowns sind auf 4:23 zu finden. 

		\subsubsection{Error Control}
			\paragraph{Error Handeling in CONNECT}
				Bleibt ein T-connect.req (CR) unbeantwortet, es kommt also zu einem Timeout, sendet der Host einfach erneut ein CR. Kommt das erste CR jedoch doch nur verspätet beim Empfänger an, und dieser Antwortet auf das erste CR mit einem CC (Connection confirmation) und bekommt dann das zweite CR, sollte durch einbeziehung des Application Layers der zweite CR unbeantwortet bleiben. Was passiert jedoch, wenn das CC verloren geht und wie geht man damit um? Denn der Empfänger (sender of CC) erwartet jetzt einen Datenaustausch. Eine Lösung wäre hier der Three-Way-Handshake

			\paragraph{Three-Way-Handshake}
				Beim Three-Way-Handshake Wird das durch ein T-Connect.rsp ausgelöste CC nochmals bestätigt. Das kann entweder direkt ber Data oder per ACK passieren. Das schützt jedoch nicht davor, dass wenn CC und ACK verloren gehen, der Empfänger nicht sagen kann, ob es sich um eine neue oder alte Anfrage handelt. Eine Lösung wäre die Einführung von Sequence Number ins CR, ACK und CC. Host A sendet eine CR (seq=x) and Host B. B antwortet mit einem ACK(seq=y,ack=x) und A sendet dann z.B. DATA(seq=x,ACK=y). Nur wenn die richtigen Sequence Numbers angehängt sind, findet dann der Datenverkehr statt. Anderer Fall. Es kommt ein alter CR bei B an, dieser Antwortet mit der seq Number, da er nicht weiß wie alt er ist. A empfängt ein ACK zu einem alten CR und sendet jedoch ein REJECT, da die Anfrage nicht mehr relevant ist. 
			
			\paragraph{Connection Rejection \& Release}
				Eine angefragte Verbindung kann jedoch auch Rejected werden. Antwortet ein Empfänger z.B. auf ein CR mit einem DR (Disconnect Request), antwortet der Sender dann mit einem DC (Disconnect Confirm). Normalerweise sendet A ein DR an B, hört jedoch auf eventuelle Daten, die noch unterwegs sein könnten. Er wartet also auf das ACK (DC) von seinem DR und kann währenddessen noch Daten Empfangen. Empfängt er ein DC, kann er sich sicher sein, dass B keine Daten danach mehr sendet. So ein Teardown nennt sich explicit. Ein Implicit Teardown wäre ein Teardown der Network Layer Connection. Das Ziel eines Release ist es, dass beide Seiten alle Daten Empfangen haben und sich nichts mehr mitzuteilen haben. Doch was passiert wenn ein DR/DC/DT verloren geht und nue gesendet werden muss? \\
				Wie kann man sicher gehen, dass A weiß was B weiß und B das weiß und A weiß dass B das weiß... 
		
		\subsubsection{Two Army Problem}
		Wie koordinieren sich zwei Armeen, die sich nur Boten durch Tal voller Feinde schicken können und wie stimmen sie ab, dass sie gemeinsam angreifen? Man kann sich nie sicher sein, dass die eigene Nachricht angekommen ist. Das ist das gleiche Problem wie beim connection Release, bei dem ist das Risiko jedoch nicht so hoch, und es können Risiken in Kauf genommen werden. Die Lösung für den Connection Release ist es, einenen Timer parallel zum DR zu starten. falls ein DC/ACK verloren geht, timeoutet die connection automatisch und beide Hosts wissen von der geschlossenen Verbindung. 

	\subsection{Flow Control}
		Bisher wurden Packete einfach per best Effort versendet. Problematisch wird es jedoch, wenn der Sender eine bessere Verbindung hat als der Empfänger. Wie kann der Sender sicher gehen ob und wenn ja welche Packete angekommen sind? Eine Bestätigung für jedes angekommene Packet vom Empfänger würde diesen eventuell noch mehr einschränken. Wird dieses Verfahren dennoch genutzt, kann der Sender einfach die Packete, die nicht geACKt wurden nach einer gewissen Zeit nochmal versenden, was den Empfänger jedoch nochmal mehr belastet. 
		Eine anderere Lösung ist Flow Control. Das Ziel ist es, langsame Empfänger vor schnellen Sendern zu schützen. Flow Control kann auf zwei Layern implementiert werden: Im Link Layer, dort wird einem Überfluss an "forwading segments" vorgegriffen. Auf höhreren Layern, z.B. auf dem Network oder Transport Layer wird vor einem Überfluss an Conections geschützt. Auf dem Link Layer ist die implementierung simpel, da Segmente eine feste Größe haben, anders auf dem Transport Layer, da können PDUs stark in der Größe variieren. 
		
		\paragraph{Buffer Allocation}
			Ein mit dem Flow Control starj zusammenhängendes Problem ist die Buffer Allocation. Ein Empfänger kann z.B. zwar nicht durch seinen Link eingeschränkt sein, sondern durch das Verarbeiten der Packete. Es wird dann ein Buffer geeigneter Größe erfordert, der eingehende Packete bis zu Bearbeitung/Weiterleitung zwischenspeichern kann. Um als Empfänger die Rate, in der Daten eingehen zu kontrollieren, gibt es verschiene Möglichkeiten. Der Empfänger verlangsamt den Sender, wenn es keinen freien Buffer Space mehr gibt (das kann explizit oder implizit geschehen). Dem kann man vorgreifen, indem der Sender initial Buffer Space anfragt und dieser dann allociert wird, oder der Empfänger announced "Ich habe noch X Buffer Space Verfügbar zur Zeit"
		
		\paragraph{Alternating-Bit-Protocol}
			Es wird angenommen, dass es genug freien Buffer Space nach dem Empfangen eines Packets gibt. Ein Empfänger sendet eine Bestätigung für jedes eingehende Packet. Der Sender warten nach jedem gesendeten Packet auf ein ACK des Empfängers. Der Empfänger sendet das ACK erst, wenn das Packet verarbeitet ist, er kann so also kontrollieren, wann und wie oft Packete eintreffen. Kommt ein ACK beim Sender nicht an, gibt es vier Fälle"
			\begin{enumerate}
				\item Packet loss
				\item ACK-loss
				\item ACK late (heavy Traffic)
				\item Ack late (Flow control)
			\end{enumerate}
			Der Sender kann jedoch nicht ziwschen den vier Fällen unterscheiden. Nach einer gewissen Zeit sendet er das DT PDU also nochmal. \\
			Das Alternating-Bit-Protocol ist in der Theorie eine gute Methode für Flow Control, aber eignet es sich auch in realen Umgebungen? In Einem Beispiel senden wir 8KBit von Frankfurt direkt nach NYC über einen 1GBit/s Link. Wir gehen davon aus, dass durch die Hosts kein Delay entsteht. Die Propagation durch die Verbindung ist
			$$
				T_{prop} \approx \frac{6200km}{300.000\frac{km}{s}} \approx 20ms
			$$ 
			Die Zeit, die das 8KBit Packet braucht, ist etwa
			$$
				T_{trans} \approx \frac{8KBit}{10^9\frac{Bit}{s}} \approx 8\mu s
			$$ 
			Weiteer nehmen wir an, das ein Ack etwa $1\mu s$ Transmission benötigt. Bis der Sender also 8Kbit übertragen hat und der Sender ein ACK PDU versendet hat und dieses beim Sender ankommt vergehen also $\approx 40.009ms$. Die Utilization des Links ist also etwa:
			$$
				U_{sender} = \frac{0,008}{40,009} \approx 0.0002
			$$
		
		\paragraph{Stop and Continue}
			Eine alternative einfache Möglichkeit ist Stop-and-Continue Methode. Der Sender sendet Daten, bis er vom Empfänger ein Stop bekommt. Er pausiert das senden bis zu einem continue Signal. Das Problem ist jedoch, dass es es bei einem überlasteten Host schwer werden kann, die Stop nachricht zu versenden, wenn es schon zu einem Overflow kommt. Auch kann es sein, dass der Sender Packete versendet, obwohl der Empfänger schon eine Stop Nachricht versendet hat, da die Nachricht noch nicht angekommen ist (Delay). Der Empfänger muss das Stop also schon früh genug versenden, um einen Overflow proaktiv zu vermeiden. Diese Methode funktioniert nur bei Full-Duplex Links. Ein Beispiel für die implementierung ist das XON/XOFF Protokoll.

		\subsubsection{Rate and Credit based Flow Control}