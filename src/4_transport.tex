\section{Transport Layer}
	In Chapter 2 beschrieben, funktioniert Routing von Packeten global am besten Abstrackt. Jedoch gibt es Anwendungen und Fälle, in denen man eine direkte Verbindung zwischen 2 Applications auf je einem Host herstellen will. Der Sender teilt Nachrichten der Application in Segmente und schickt diese weiter an den Network Layer, um zum Ziel geroutet zu werden. Im Network Layer des Empfängers werden die Segmente dann zu Nachrichten zusammengebaut und an die App weitergereicht. \\
	Im Transport Layer wird die logische Verbindung zwischen Prozessen hergestellt, während im Vergleich der Network Layer die logische Verbindung zwischen Hosts herstellt. Im Internet sind Beispiele für Protokolle des Transport Layers TCP \& UDP. 
	\begin{itemize}
		\item TCP
			\begin{itemize}
				\item Congestion Control
				\item Flow Control
				\item connection Setup \& teardown  (COTS)
			\end{itemize}
		\item UDP  
			\begin{itemize}
				\item connectionless
				\item Best-effort implemtierung von IP auf dem Transport Layer
			\end{itemize}
	\end{itemize}
	Keines dieser Protokolle bietet jedoch eine Lösung für Delay \& Bandwidth guarantees.

	\subsection{Segmentation(+Blocking). Multiplexing, Addressing}
		Die theoretische beschränkung von IP Packeten sind 64k mit Header. TCP Streams sind zum Teil jedoch längern. Um Applications ein Verständnis zu geben, wie Packete verschickt und SDUs zusammengebaut werden, muss dieser Vorang transparent geschehen. Packete können über Multiplexing auf dem Host an alle Applications gesendet werden, und Applications holen sich nur die Packete, die sie brauchen. Das kommt jedoch mit einem großen Sicherheitsproblem, da nun Anwendungen wie Trojaner einfach den Traffic anderer Programme mithören. 

		\paragraph{Sockets and Ports} 
			Das OSI Modell sieht für die Kommunikation zwischen Programmen und zur identifizierung dieser die Nutzung von lokalen CEPs vor. In Systemen könnte der System PID zur identifizierung benutzt werden, der müsste jedoch für alle Hosts erreichbar sein, außerdem ändert sich dieser bei jedem Neustart des Prozesses oder des Hosts. Im Internet Modell hat man sich jedoch für die Einführung von Ports entschieden. Ein Prozess kann auf einen festen Port announcen oder nach einem zufälligen Port fragen. \\
			Eine Application ist dann durch einen socket (IP:Port Kombination) erreichbar, wobei die IP die des Empfänger Hosts und die Port Nummer die der Empfänger Application ist. Es gibt gewissen Ports, die für Anwendungen reserviert sind a.k.a. ''well-known'' Ports (IANA: 0-1023). Beispiele sind 22/tcp für ssh, 25/tcp für smtp oder 443/tcp für https. Prozesse die solche Ports öffnen wollen brauchen in der Regel root Privilegien. \\ 
			Um demultiplexing im Internet zu nutzen, empfängt der Host ein IP Datagram, bestehend aus source und destionation Socket. Diese Datagrams transportieren je ein Transport-Layer segment als Application Data. Das Problem hierbei: Wenn IP eines Tages ersetzt wird, funktionieren TCP/UDP nicht mehr, da die Nutzung von IP:Port hard in die implemtierung gecoded sind. 
	
	\subsection{Connection Controll}
		Connection Control funktioniert natürlich nur bei CO Protokollen, in unserem Beispiel also nur TCP. Wie schon in Kapitel 1 angeschnitten, gibt es 3 Phasen der einer connection:
		\begin{itemize}
			\item CONNECT (herstellen der Verbindung)
			\item DATA (Transfer von Daten)
			\item DISCONNECT (Schließen der Verbindung)
		\end{itemize}
		Um zwischen Network und Transport Layer Phasen zu unterscheiden, schreibt man dort jeweils ein N-- bzw. T- Prefix zu den Phasen also z.B. T-Connect oder N-Data.

		\subsubsection{Phases}  
			Die verschiedenen Schritte können confirmed oder unconfirmed ablaufen. T-Connect wird immer confirmed, T-Data wird unconfirmed übertragen (in seltenden Fällen kann das auch confirmed laufen) und T-Disconnect können confirmed und unconfirmed ablaufen. Ein Beispiel für ein Ablauf ist auf 4:20 zu finden. Auch ein State-Diagramm, dass die Übergänge zu verschiedenen State beschreibt ist auf 4:24 enthalten.

			\paragraph{Establishment}
				Beim T-Connect gibt es mehrere Methoden, die aufgerufen werden (können)
				\begin{itemize}
					\item T-Connect.Request(Destination Address, Source Address)
					\item T-Connect.Indication(Destination Address, Source Address)
					\item T-Connect.Response(Responding Address)
					\item T-Connect.Confirmation(Responding Address)
				\end{itemize}
				Wobei die Destination Address die Addresse des Transport Service Users ist, also die Addresse die zum Transport gerufen wird. Die Source Address ist die Addresse des aufrufenden Service Users und die Responding Address die Addresse des Antwortenden Service Users. 

			\paragraph{Data Transfer}
				\begin{itemize}
					\item T-Data.req(userdata)
					\item T-Data.ind(userdata)
				\end{itemize}
				Die userdata ist dabei die Payload, die von der Application transportiert werden soll. 

			\paragraph{Connection Release}
				Der Grund für einen Disconnect kann verschieden sein. Es kann zu einem Release kommen durch abruptes verlieren der Verbding oder als Folge des Connects. Das verlieren von TSDUs (T + SDU) ist möglich.
				\begin{itemize}
					\item T-Disconnect.req(userdata)
					\item T-Disconnect.ind(cause, userdata)
				\end{itemize}
				Der cause für einen Disconnect kann z.B. ein request vom remote User, Quality of service below minimum, error oder auch unknowm sein. Beispiele für Abläufe verschiedener teardowns sind auf 4:23 zu finden. 

		\subsubsection{Error Control}
			\paragraph{Error Handeling in CONNECT}
				Bleibt ein T-connect.req (CR) unbeantwortet, es kommt also zu einem Timeout, sendet der Host einfach erneut ein CR. Kommt das erste CR jedoch doch nur verspätet beim Empfänger an, und dieser Antwortet auf das erste CR mit einem CC (Connection confirmation) und bekommt dann das zweite CR, sollte durch einbeziehung des Application Layers der zweite CR unbeantwortet bleiben. Was passiert jedoch, wenn das CC verloren geht und wie geht man damit um? Denn der Empfänger (sender of CC) erwartet jetzt einen Datenaustausch. Eine Lösung wäre hier der Three-Way-Handshake

			\paragraph{Three-Way-Handshake}
				Beim Three-Way-Handshake Wird das durch ein T-Connect.rsp ausgelöste CC nochmals bestätigt. Das kann entweder direkt ber Data oder per ACK passieren. Das schützt jedoch nicht davor, dass wenn CC und ACK verloren gehen, der Empfänger nicht sagen kann, ob es sich um eine neue oder alte Anfrage handelt. Eine Lösung wäre die Einführung von Sequence Number ins CR, ACK und CC. Host A sendet eine CR (seq=x) and Host B. B antwortet mit einem ACK(seq=y,ack=x) und A sendet dann z.B. DATA(seq=x,ACK=y). Nur wenn die richtigen Sequence Numbers angehängt sind, findet dann der Datenverkehr statt. Anderer Fall. Es kommt ein alter CR bei B an, dieser Antwortet mit der seq Number, da er nicht weiß wie alt er ist. A empfängt ein ACK zu einem alten CR und sendet jedoch ein REJECT, da die Anfrage nicht mehr relevant ist. 
			
			\paragraph{Connection Rejection \& Release}
				Eine angefragte Verbindung kann jedoch auch Rejected werden. Antwortet ein Empfänger z.B. auf ein CR mit einem DR (Disconnect Request), antwortet der Sender dann mit einem DC (Disconnect Confirm). Normalerweise sendet A ein DR an B, hört jedoch auf eventuelle Daten, die noch unterwegs sein könnten. Er wartet also auf das ACK (DC) von seinem DR und kann währenddessen noch Daten Empfangen. Empfängt er ein DC, kann er sich sicher sein, dass B keine Daten danach mehr sendet. So ein Teardown nennt sich explicit. Ein Implicit Teardown wäre ein Teardown der Network Layer Connection. Das Ziel eines Release ist es, dass beide Seiten alle Daten Empfangen haben und sich nichts mehr mitzuteilen haben. Doch was passiert wenn ein DR/DC/DT verloren geht und nue gesendet werden muss? \\
				Wie kann man sicher gehen, dass A weiß was B weiß und B das weiß und A weiß dass B das weiß... 
		
		\subsubsection{Two Army Problem}
		Wie koordinieren sich zwei Armeen, die sich nur Boten durch Tal voller Feinde schicken können und wie stimmen sie ab, dass sie gemeinsam angreifen? Man kann sich nie sicher sein, dass die eigene Nachricht angekommen ist. Das ist das gleiche Problem wie beim connection Release, bei dem ist das Risiko jedoch nicht so hoch, und es können Risiken in Kauf genommen werden. Die Lösung für den Connection Release ist es, einenen Timer parallel zum DR zu starten. falls ein DC/ACK verloren geht, timeoutet die connection automatisch und beide Hosts wissen von der geschlossenen Verbindung. 

	\subsection{Flow Control}
		Bisher wurden Packete einfach per best Effort versendet. Problematisch wird es jedoch, wenn der Sender eine bessere Verbindung hat als der Empfänger. Wie kann der Sender sicher gehen ob und wenn ja welche Packete angekommen sind? Eine Bestätigung für jedes angekommene Packet vom Empfänger würde diesen eventuell noch mehr einschränken. Wird dieses Verfahren dennoch genutzt, kann der Sender einfach die Packete, die nicht geACKt wurden nach einer gewissen Zeit nochmal versenden, was den Empfänger jedoch nochmal mehr belastet. 
		Eine anderere Lösung ist Flow Control. Das Ziel ist es, langsame Empfänger vor schnellen Sendern zu schützen. Flow Control kann auf zwei Layern implementiert werden: Im Link Layer, dort wird einem Überfluss an "forwading segments" vorgegriffen. Auf höhreren Layern, z.B. auf dem Network oder Transport Layer wird vor einem Überfluss an Conections geschützt. Auf dem Link Layer ist die implementierung simpel, da Segmente eine feste Größe haben, anders auf dem Transport Layer, da können PDUs stark in der Größe variieren. 
		
		\paragraph{Buffer Allocation}
			Ein mit dem Flow Control starj zusammenhängendes Problem ist die Buffer Allocation. Ein Empfänger kann z.B. zwar nicht durch seinen Link eingeschränkt sein, sondern durch das Verarbeiten der Packete. Es wird dann ein Buffer geeigneter Größe erfordert, der eingehende Packete bis zu Bearbeitung/Weiterleitung zwischenspeichern kann. Um als Empfänger die Rate, in der Daten eingehen zu kontrollieren, gibt es verschiene Möglichkeiten. Der Empfänger verlangsamt den Sender, wenn es keinen freien Buffer Space mehr gibt (das kann explizit oder implizit geschehen). Dem kann man vorgreifen, indem der Sender initial Buffer Space anfragt und dieser dann allociert wird, oder der Empfänger announced "Ich habe noch X Buffer Space Verfügbar zur Zeit"
		
		\paragraph{Alternating-Bit-Protocol}
			Es wird angenommen, dass es genug freien Buffer Space nach dem Empfangen eines Packets gibt. Ein Empfänger sendet eine Bestätigung für jedes eingehende Packet. Der Sender warten nach jedem gesendeten Packet auf ein ACK des Empfängers. Der Empfänger sendet das ACK erst, wenn das Packet verarbeitet ist, er kann so also kontrollieren, wann und wie oft Packete eintreffen. Kommt ein ACK beim Sender nicht an, gibt es vier Fälle"
			\begin{enumerate}
				\item Packet loss
				\item ACK-loss
				\item ACK late (heavy Traffic)
				\item Ack late (Flow control)
			\end{enumerate}
			Der Sender kann jedoch nicht ziwschen den vier Fällen unterscheiden. Nach einer gewissen Zeit sendet er das DT PDU also nochmal. \\
			Das Alternating-Bit-Protocol ist in der Theorie eine gute Methode für Flow Control, aber eignet es sich auch in realen Umgebungen? In Einem Beispiel senden wir 8KBit von Frankfurt direkt nach NYC über einen 1GBit/s Link. Wir gehen davon aus, dass durch die Hosts kein Delay entsteht. Die Propagation durch die Verbindung ist
			$$
				T_{prop} \approx \frac{6200km}{300.000\frac{km}{s}} \approx 20ms
			$$ 
			Die Zeit, die das 8KBit Packet braucht, ist etwa
			$$
				T_{trans} \approx \frac{8KBit}{10^9\frac{Bit}{s}} \approx 8\mu s
			$$ 
			Weiteer nehmen wir an, das ein Ack etwa $1\mu s$ Transmission benötigt. Bis der Sender also 8Kbit übertragen hat und der Sender ein ACK PDU versendet hat und dieses beim Sender ankommt vergehen also $\approx 40.009ms$. Die Utilization des Links ist also etwa:
			$$
				U_{sender} = \frac{0,008}{40,009} \approx 0.0002
			$$
		
		\paragraph{Stop and Continue}
			Eine alternative einfache Möglichkeit ist Stop-and-Continue Methode. Der Sender sendet Daten, bis er vom Empfänger ein Stop bekommt. Er pausiert das senden bis zu einem continue Signal. Das Problem ist jedoch, dass es es bei einem überlasteten Host schwer werden kann, die Stop nachricht zu versenden, wenn es schon zu einem Overflow kommt. Auch kann es sein, dass der Sender Packete versendet, obwohl der Empfänger schon eine Stop Nachricht versendet hat, da die Nachricht noch nicht angekommen ist (Delay). Der Empfänger muss das Stop also schon früh genug versenden, um einen Overflow proaktiv zu vermeiden. Diese Methode funktioniert nur bei Full-Duplex Links. Ein Beispiel für die implementierung ist das XON/XOFF Protokoll.

		\subsubsection{Rate and Credit based Flow Control}
			Beim Rate based Flow Control können im Vergleich zum Stop and Continue Änderungen in der Rate angekündigt werden. Wenn sich also der maximal Verfügbare Buffer ändert kann der Empfänger das mitteilen. Jedoch auch hier gibt es das Problem, dass der Sender erst zu spät davon mitbekommt. Der Übergang zwischen Phasen kann anders als bei Stop and Continue flüssiger ablaufen, da der Sender nicht direkt den Empfang stoppen muss, sondern ihn auch nur einschränken kann. Es gibt jedoch keine Möglichkeit für den Sender sicherzustellen, dass Packete ankommen. \\
			Idee des Credit Based Flow Control ist es, dem Sender regelmäßig Credits zu geben, um Packete zu senden. Hat der Sender keine Credits mehr, pausiert er das senden, bis er wieder neue credits vom Empfänger bekommt. Alternativ kann der Sender auch einmalig eine absolute Anzahl bekanntgeben. \\
			Auch lassen sich die Konzepte mit einer Error correction kombinieren. Der Sender kann dem Empfänger "permitten", daten zu senden und er kann das empfangen von Daten "acknowledgen". Eine ACK Nachricht beduetet dabei, die Nachricht ist korrept angekommen. Der Sender kann also seinen timoute-timer sowie die lokale Kopie des Packets löschen. Problem ist jedoch, wenn durch Flow Control das ACK Packet ausbleibt. Wenn der Sender timeoutet, sendet er das Packet dann nochmal. \\
			Eine Protokoll des Credit Based Flow Control ist das Sliding Window Protokoll. Ein Beispiel ist auf 4:46. 
			
	\subsection{Error Control}
		Es kann nie garantiert werden, dass alle Packete ankommen und noch weniger, dass das immer erkannt wird. Wird jedoch erkannt, dass ein Packet verloren gegangen ist, gibt es bei den meisten Protokollen keine Reaktion. Das Packet wird einfach gedroppt. Meistens wird das durch eine Fehlerhafte Checksumme im Header erkannt. Da die Checksumme für den IP Header jedoch die gleiche wie für die Payload ist, ensteht die meiste Fehlererkennung an der Stelle, kann jedoch falsch Interpretiert werden. Ein Empfänger kann auch beim feststellen dass ein Packet Fehlerhaft war ein negative-ACK versenden und die neusendung anfordern. Das hat den Vorteil, dass dann nicht erst der timeout beim Sender auslaufen muss, bis das Packet neu verschickt wird. Wird ein Fehler z.B. durch eine falsche Sequence Number erkannt und ein NACK versendet, kann es jedoch sein, dass ein Packet über einen anderen Weg geroutet wurde, weil ein schnellerer gefunden wurde, und das vorherige Packet noch z.B. im Router hängt. Einzig verlässlig in dem Punkt ist das System: timeout @sender → resend. 

		\paragraph{Automatic Repeat reQuest (ARQ)}
			Der erste Typ ARQ war das Alternating Bit Protocol. Das lässt sich jedoch weiter verbessern: Go-back-N. Geht ein Packet z.B. mit der Sequence Number 2 verloren oder löst ein Error aus, sendet der Empfänger kein ACK für diesen Frame. Er sendet jedoch weiter die Folge Frames (z.B. 3-8), dessen Frames vom Empfänger Link jedoch weggeworfen werden, da dieser noch auf den Frame mit der Seq. Number 2 warten. Timeoutet dann der Frame2, wird dieser erneut vom Sender gesendet. der Empfänger empfängt diesen Frame korrekt und sendet dann ein ACK. währenddessen sendet der Sender die Folge Frames von 2 (3-8) ebenfalls nochmals da diese auch kein ACK bekommen haben. (Beispiel siehe 4:49). \\ Eine Adaption dieses Protokolls wäre, wenn der Empfänger nicht auf den Timeout wartet, sondern für den fehlerhaften Frame ein NACK sendet. Dieser Frame wird dann erneut gesendet, in der Zeit buffert der data link des Empfängers Frames mit dn folgende seq. Number.

	\subsection{Congestion Control}
		Bei Kommunikation zwischen zwei Hosts gibt es immer ein Bottleneck, dass die Geschwindigkeit beschränkt. Ein Netz ist dann auf genau dieses Bottleneck beschränkt. Werden mehr Daten gesendet, als das Bottleneck übertragen kann, kommt es zu einem congestion collapse. Bei solch einem collapse gehen viele Packete verloren und die Effizienz sinkt unter das Bottleneck. Je nachdem wo das Bottleneck ist, kann sich das ganze auch in sich selbst steigern, ist z.B. der Router durch den Buffer Space beschränkt und es kommt zu einem collapse wobei der Sender ein Protokoll nutzt, dass nicht angekommene Packete nochmal sendet, wird der Router mit einer noch größeren Flut an Packeten überschwemmt.
		Im folgenden sind $\lambda_{in}$ die Daten die Host A versendet und $\lambda_{out}$ die, die Host B empfängt. Im Idealfall ohne Packetverlust ist $\lambda_{in}=\lambda_{out}$
		\paragraph{Scenario 1}
			Wir gehen von einem Router mit umbegränztem Buffer aus und Packete werden nicht neu versendet. Die Throughput steigt bis zum Maximum an, bleibt dann stabil, der Delay steigt exponentiell an, wenn es zu einer congestion kommt.
		
		\paragraph{Scenario 2}
			Hat der Router jedoch einen beschränkten Buffer und Host A sendet nicht bestätigte Packete nach einiger Zeit nochmal. Host A sendet jetzt $\lambda'_{in}=\lambda_{in}$+ retransmitted packages. Perfekt wäre nur wirklich verloren gegangene Packete neu gesendet werden, $\lambda'_{in}$ wäre $>\lambda_{out}$. Werden nicht wirklich verlorerene Packete neu gesendet, wird $\lambda'_{in}$ hörher als nötig. 

		Wie bei den Scenarien gemerkt, wird congestion controll esentiell, um einen Schneeball Effekt zu verhindern. 
		
		\subsubsection{solving congestion Control}
			Eine Globale Lösung wäre, die Sende Rate an den Bottleneck anzupassen. Das kann jedoch nur gloabl passieren und jeder Router müsste seine Maximal Kapazität bekanntgeben. Diese Lösung macht das Problem nur komplizierter und kleine Konfigurationsfehler können das gesamte System betreffen. Deswegen wird Congestion Control zu einem lokalen Problem gemacht. Meistens wird es zwischen Sender um Empfänger gelöst. 
			Das Ziel ist es, so viele Packete wie möglich zu verschicken, ohne jedoch die Leitung zu verstopfen. Außerdem sollte Congestion Control Fair ablaufen, jedoch unter welchen Bedingungen? Ein 3-Hop Weg ist genausoviel Wert wie 3 1-Hop ways? Priorisierung in Anwendungen (Video Call > Downloads)? 

			\paragraph{Design Options for Congestion Control}
				Es gibt 2 fundamentale Anstätze, Congestion Control zu gewährleisten:
				\begin{itemize}
					\item Closed-Loop - Der Sender bekommt Feedback über das aktuelle Verhalten und kann die Kapazität anpassen. 
						\begin{itemize}
							\item implementierung am Sender
							\item implementierung am Empfänger
						\end{itemize}
					\item Open-Loop - Das System von vorne herein funktioniert und keine Korrekturen im Betrieb notwenig sind.
						\begin{itemize}
							\item explicit Feedback - Im Fall wenn es passiert, den Sender Informieren
							\item implizit Feedback - Der Sender passt die Rate selbst an ohne direktes Feedback vom Empfänger sondern durch Daten aus dem Betrieb (z.B. ausbleibende ACKs). Beispeil: TCP
						\end{itemize}
				\end{itemize}
			
			\paragraph{Possible Actions}
				Um Netze vor Congestion zu schützen, gibt es andere alternativen als einfach den Traffic zu pausieren. Um häufige Congestions entgegen zu wirken, kann die Netz Kapazität z.B. durch mehr Router erhöht werden. Auch kann ein Router das allocieren von zusätzlichem Traffic einschränken, wenn das Netz schon fast ausgelastet ist, jedoch sind Informationen über den Netzwerk state selten Verfügbar. Ebenso können alle Session einen Teil der Verfügbaren Load reduzieren, um allgemein mehr Kapazität zu schaffen. Dafür wird jedoch Netzwerk Feedback benötigt, das geht also nur in closed-loops. \\
				Die Klassifizierung der Packete kann Zentral am Router oder am Host stattfinden. Im Internet geschieht das (außer beim droppen von Packeten) am End-Host. Die einzelnen Klassen bekommen entweder einen gewissen Byte Betrag pro Sekunde (rate based) oder eine gewisse Anzahl an Sequence Numbers/Bytes im Netz verarbeitet werden dürfen (Es gibt auch andere, jedoch unpopulärere Methoden z.B. Credit Based Congestion Control). \\
			
			\paragraph{Package Dropping}
				Ensteht ein collapse durch einen vollen Router Buffer, müssen Packete aus diesem gedroppt werden, doch welche? Eine Methode ist die drop-tail queue, bei dir das neuse Packet weggeworfen wird. Das spielt z.B. gut mit dem go-back-n Protokoll zusammen. Für andere Anwendungen, z.B. Telefon sind neue Packete wichtiger als alte, hier würden am besten Packete gedroppt, die schon lange in der Queue sind. Eine solche Aktion ist auch immer, ein implizites Feedback. Der Sender erkennt das nicht ankommen des Packets. Ensteht eine Congestion durch einen vollen Buffer, muss der Traffic verringert werden. 
			
			\paragraph{proactiv Actions}
				Prinzipiell ist ein Router nicht mehr in der Lage, richtig zu funktionieren, wenn er einmal eine volle Queue hat, es sollte also um jeden Preis vermieden werden, z.B. indem man bereits bei z.B. 90\% einen warning state Aufruft. Eine andere Methode ist das Chocke Packet. Ein Router mit vollen Buffer sendet so ein solches Packet an den Sender, dass diesem sagt, die Rate zu verringern. Das Problem ist jedoch, in einem congested Netz gehen mehr Packete verloren als sonst, es kann also sein, dass der Sender ein solches Packet nie erhält. Auch kann ein Router statt ein extra Packet zu senden, ein warning Bit in allen ausgehenden Packeten setzen, dass allen Hosts sagt, dass der Router überlastet ist, oder das kurz bevor steht. Eine vielleicht etwas radikalere Lösung ist die Random Early Detection (RED), die mit einer Rate zufällig Packete droppt, auch wenn der Router noch nicht überlastet ist, um diesen genau davor zu schützen. Diese Rate je nach Anzahl Packete im Buffer variieren. 

			\paragraph{Feedback Actions}
				Sobald ein Sender Informationen über eine congestion erreicht hat, muss der Traffic reduziert werden. Doch wie das geschieht, hängt von den Protokollen des Transport Layers ab.

	\subsection{UDP}
		%4:73